{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2cjqQyUImuYrWxyHvNCsX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CaptainPlusPlus/btba_reproduction/blob/main/btba_reproduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reproduction of BTBA Model for Unsupervised Word Alignment\n",
        "Article can be found here: https://aclanthology.org/2021.acl-long.24.pdf"
      ],
      "metadata": {
        "id": "ExMtdui5y-3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Requirements\n",
        "* Downloand and preprocess the deen, fren, roen texts from https://github.com/lilt/alignment-scripts.\n",
        "* Upload the `bpe` lowercased preprocessed `train` & `test` folders as well as the sentencepiece `bpe` models."
      ],
      "metadata": {
        "id": "SmCCmEo3yXov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Tokenizer Definition\n",
        "\n",
        "Since the Sentencepiece tokenizer used in the article and alignment scripts it is compared against outputs a binary format model and vocabulary, the sentencepiece tokenizer must be adjusted to fit the HuggingFace models used to reproduce the article's transformer based approaach."
      ],
      "metadata": {
        "id": "Ap-m1E74yOte"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0_1oRQYyyGXl"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "class CustomSentencePieceTokenizer:\n",
        "    def __init__(self, sentencepiece_model_path):\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        if not self.sp.Load(sentencepiece_model_path):\n",
        "            raise FileNotFoundError(\"Failed to load SentencePiece model from specified path.\")\n",
        "        self.special_tokens = {'<s>': self.sp.piece_to_id('<s>'), '</s>': self.sp.piece_to_id('</s>'), '<unk>': self.sp.piece_to_id('<unk>')}\n",
        "        self.additional_special_tokens = {'<pad>': self.sp.GetPieceSize(), '<mask>': self.sp.GetPieceSize() + 1}\n",
        "        self.special_token_ids = {**self.special_tokens, **self.additional_special_tokens}\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return self.sp.encode_as_pieces(text)\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        return [self.special_token_ids.get(token, self.sp.piece_to_id(token)) for token in tokens]\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "        id_to_token_map = {id: token for token, id in self.special_token_ids.items()}\n",
        "        id_to_token_map.update({id: self.sp.id_to_piece(id) for id in range(self.sp.GetPieceSize())})\n",
        "        return [id_to_token_map.get(id, '<unk>') for id in ids]\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.sp.GetPieceSize() + len(self.additional_special_tokens)\n",
        "\n",
        "    def get_special_tokens(self):\n",
        "        return {**self.special_tokens, **self.additional_special_tokens}\n",
        "\n",
        "    def get_special_token_ids(self):\n",
        "        return self.special_token_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_custom_tokenizer():\n",
        "    tokenizer = CustomSentencePieceTokenizer('/content/drive/MyDrive/bachelor_thesis/data/alignment-scripts/train/bpe.deen.model')\n",
        "    test_sentence = \"das ist ein test.\"\n",
        "    print(\"Testing tokenization of sentence:\", test_sentence)\n",
        "    tokens = tokenizer.tokenize(test_sentence)\n",
        "    print(\"Tokens:\", tokens)\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    print(\"Token IDs:\", token_ids)\n",
        "    tokens_from_ids = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "    print(\"Tokens from IDs:\", tokens_from_ids)\n",
        "    print(\"Special Tokens:\", tokenizer.get_special_tokens())\n",
        "    print(\"Special Token IDs:\", tokenizer.get_special_token_ids())\n",
        "    special_tokens_test = ['<pad>', '<mask>', '<s>', '</s>', '<unk>']\n",
        "    special_tokens_ids = tokenizer.convert_tokens_to_ids(special_tokens_test)\n",
        "    print(\"Special tokens to IDs:\", list(zip(special_tokens_test, special_tokens_ids)))\n",
        "    special_tokens_round_trip = tokenizer.convert_ids_to_tokens(special_tokens_ids)\n",
        "    print(\"IDs back to special tokens:\", special_tokens_round_trip)\n",
        "\n",
        "test_custom_tokenizer()"
      ],
      "metadata": {
        "id": "vu6WLpwz04Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = CustomSentencePieceTokenizer(model_path)"
      ],
      "metadata": {
        "id": "3WFG1iWM0xq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load bpe lowercased data and save tokenized data"
      ],
      "metadata": {
        "id": "-58ExfKzz-QH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def load_data(src_file, tgt_file):\n",
        "    with open(src_file, 'r', encoding='utf-8') as src_f, open(tgt_file, 'r', encoding='utf-8') as tgt_f:\n",
        "        src_lines = [line.strip() for line in src_f.readlines()]\n",
        "        tgt_lines = [line.strip() for line in tgt_f.readlines()]\n",
        "    assert len(src_lines) == len(tgt_lines), \"Source and target files should have the same number of lines.\"\n",
        "    return src_lines, tgt_lines\n",
        "\n",
        "def tokenize_and_save_data(src_lines, tgt_lines, tokenizer, src_path, tgt_path):\n",
        "    tokenized_src = [torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(line))) for line in src_lines]\n",
        "    tokenized_tgt = [torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(line))) for line in tgt_lines]\n",
        "    torch.save(tokenized_src, src_path)\n",
        "    torch.save(tokenized_tgt, tgt_path)\n",
        "    print(f\"Tokenized data saved to {src_path} and {tgt_path}\")\n"
      ],
      "metadata": {
        "id": "mVWKvpQUz81I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_file_path = '/content/drive/MyDrive/bachelor_thesis/data/alignment-scripts/train/deen.lc.plustest.src.bpe'\n",
        "tgt_file_path = '/content/drive/MyDrive/bachelor_thesis/data/alignment-scripts/train/deen.lc.plustest.tgt.bpe'\n",
        "model_path = '/content/drive/MyDrive/bachelor_thesis/data/alignment-scripts/train/bpe.deen.model'\n",
        "tokenized_src_path = '/content/drive/MyDrive/bachelor_thesis/data/alignment-scripts/train/deen_tokenized_src.pt'\n",
        "tokenized_tgt_path = '/content/drive/MyDrive/bachelor_thesis/data/alignment-scripts/train/deen_tokenized_tgt.pt'\n",
        "\n",
        "src_lines, tgt_lines = load_data(src_file_path, tgt_file_path)\n",
        "tokenize_and_save_data(src_lines, tgt_lines, tokenizer, tokenized_src_path, tokenized_tgt_path)"
      ],
      "metadata": {
        "id": "AQL1Hr350ns3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartConfig, BartModel\n",
        "import torch.nn as nn\n",
        "\n",
        "class BTBADecoderLayer(nn.Module):\n",
        "    def __init__(self, config, include_ffn=True):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(config.d_model, config.decoder_attention_heads)\n",
        "        self.multihead_attn = nn.MultiheadAttention(config.d_model, config.decoder_attention_heads)\n",
        "        self.layer_norm1 = nn.LayerNorm(config.d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(config.d_model)\n",
        "        self.include_ffn = include_ffn\n",
        "        if include_ffn:\n",
        "            self.ffn = nn.Sequential(\n",
        "                nn.Linear(config.d_model, config.decoder_ffn_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(config.decoder_ffn_dim, config.d_model),\n",
        "            )\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, memory, src_mask=None, tgt_mask=None):\n",
        "        x = self.layer_norm1(x + self.dropout(self.self_attn(x, x, x, key_padding_mask=tgt_mask)[0]))\n",
        "        x = self.layer_norm2(x + self.dropout(self.multihead_attn(x, memory, memory, key_padding_mask=src_mask)[0]))\n",
        "        if self.include_ffn:\n",
        "            x = self.ffn(x)\n",
        "        return x\n",
        "\n",
        "class BTBAModel(BartModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        assert hasattr(config, 'decoder_ffn_dim'), \"decoder_ffn_dim is not defined in the configuration\"\n",
        "        self.decoder.layers = nn.ModuleList([\n",
        "            BTBADecoderLayer(config, include_ffn=(i < config.decoder_layers - 1))\n",
        "            for i in range(config.decoder_layers)\n",
        "        ])"
      ],
      "metadata": {
        "id": "vVYAdMJb1s1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic Masking for tje Dataset\n",
        "\n",
        "* Every word in a sentence should be masked only once across the entire training - track the masking state and reset it after each epoch.\n",
        "* Percentage-based: At least 10% of the words in each sentence must be masked, or one word if the sentence has less than ten words."
      ],
      "metadata": {
        "id": "ImlkdDaf19vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DynamicMaskingDataset(Dataset):\n",
        "    def __init__(self, tokenized_src, tokenized_tgt, tokenizer, mask_probability=0.1):\n",
        "        self.tokenized_src = tokenized_src\n",
        "        self.tokenized_tgt = tokenized_tgt\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mask_id = tokenizer.get_special_token_ids()['<mask>']\n",
        "        self.pad_id = tokenizer.get_special_token_ids()['<pad>']\n",
        "        self.mask_probability = mask_probability\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_tgt)\n",
        "\n",
        "    def mask_input(self, inputs):\n",
        "        num_tokens = len(inputs)\n",
        "        num_to_mask = max(int(num_tokens * self.mask_probability), 1)\n",
        "\n",
        "        # Don't mask special tokens\n",
        "        candidate_mask = (inputs != self.pad_id) & (inputs != self.tokenizer.get_special_token_ids()['<s>']) & (inputs != self.tokenizer.get_special_token_ids()['</s>'])\n",
        "        candidate_indices = np.where(candidate_mask.numpy())[0]\n",
        "\n",
        "        masked_indices = np.random.choice(candidate_indices, size=num_to_mask, replace=False)\n",
        "        labels = inputs.clone()\n",
        "\n",
        "        inputs[masked_indices] = self.mask_id\n",
        "        labels[candidate_mask == 0] = -100  # No loss for unmaksed tokens\n",
        "\n",
        "        return inputs, labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.tokenized_src[idx]\n",
        "        tgt = self.tokenized_tgt[idx]\n",
        "        src, _ = self.mask_input(src)\n",
        "        tgt, labels = self.mask_input(tgt)\n",
        "        return {\"input_ids\": src, \"labels\": labels}\n"
      ],
      "metadata": {
        "id": "l-JX0jEF1813"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_src = torch.load(tokenized_src_path)\n",
        "tokenized_tgt = torch.load(tokenized_tgt_path)\n",
        "\n",
        "dataset = DynamicMaskingDataset(tokenized_src, tokenized_tgt, tokenizer)"
      ],
      "metadata": {
        "id": "S480mEuc2s-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartConfig\n",
        "\n",
        "config = BartConfig.from_pretrained('facebook/bart-large')\n",
        "config.decoder_ffn_dim = 3072\n",
        "model = BTBAModel(config)"
      ],
      "metadata": {
        "id": "6c4_MsZp3EOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "O40bT8gN245V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}