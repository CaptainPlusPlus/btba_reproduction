{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creating and optimizng the BTBA Tranformer model",
   "id": "8fb6792d38e3dcae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocessing data\n",
    "Using Ro<->En, De<->En and Fr<->En datasets from https://github.com/lilt/alignment-scripts/tree/master to compare with other works.\n",
    "Preprocessed as described in the repository, and used here under /data"
   ],
   "id": "7af20a2db79d1c4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Masking input\n",
    "\n",
    "* 10% of the data is masked (at least one word per sentence).\n",
    "* Every token is masked exactly once. \n",
    "* masking sentences is done on the fly in every epoch.\n",
    "* Following Och and Ney (2003), <bos> token is added at the beginning of source sentence for target words without alignments in source."
   ],
   "id": "70b0169edca70368"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T15:11:58.414139Z",
     "start_time": "2024-05-12T15:11:58.410002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Masking Dependencies\n",
    "import random\n",
    "import itertools"
   ],
   "id": "8e1da11417ca6a6f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T15:12:02.205874Z",
     "start_time": "2024-05-12T15:12:02.200380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_mask(sentence, mask_index, mask_token=\"<x>\"):\n",
    "    \"\"\" Replace word at specified index with mask token. \"\"\"\n",
    "    words = sentence.strip().split()\n",
    "    words[mask_index] = mask_token\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# def generate_masked_sentences(sentence, mask_token=\"<x>\"):\n",
    "#     \"\"\" Generate permutations of sentence with mask token. \"\"\"\n",
    "#     words = sentence.strip().split()\n",
    "#     num_words = len(words)\n",
    "# \n",
    "#     indices = list(range(num_words))\n",
    "#     random.shuffle(indices)\n",
    "# \n",
    "#     # Distribute masked indices evenly with into at most 10 sets.\n",
    "#     num_sets = min(num_words, 10)\n",
    "#     for i in range(num_sets):\n",
    "#         current_indices = indices[i::num_sets]\n",
    "#         if not current_indices:\n",
    "#             continue\n",
    "#         masked_sentence = words[:]\n",
    "#         for index in current_indices:\n",
    "#             masked_sentence[index] = mask_token\n",
    "#         yield ' '.join(masked_sentence)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare data for training",
   "id": "789f4a12bdfd8d77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T16:03:35.557795Z",
     "start_time": "2024-05-12T16:03:35.555892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import combinations\n",
    "import random"
   ],
   "id": "560ea2d942cf7450",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T16:03:12.047958Z",
     "start_time": "2024-05-12T16:03:09.099738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_masked_sentences(sentence, mask_token=\"<mask>\", max_outputs=10):\n",
    "    words = sentence.strip().split()\n",
    "    num_words = len(words)\n",
    "    # Assuming the first word is '<bos>' and should not be masked\n",
    "    num_to_mask = max(1, int((num_words - 1) * 0.1))  # Exclude the first token from the percentage calculation\n",
    "    index_permutations = list(combinations(range(1, num_words), num_to_mask))  # Start range at 1 to exclude '<bos>'\n",
    "    random.shuffle(index_permutations)  # Shuffle to ensure random selection\n",
    "\n",
    "    masked_sentences = []\n",
    "    for indices_to_mask in index_permutations[:max_outputs]:\n",
    "        temp_words = words[:]\n",
    "        for index in indices_to_mask:\n",
    "            temp_words[index] = mask_token\n",
    "        masked_sentences.append(' '.join(temp_words))\n",
    "    return masked_sentences\n",
    "\n",
    "\n",
    "# Sample usage with preprocessed data\n",
    "src_train_data = [\"<bos> \" + line.strip() for line in open(\"data/alignment-scripts/train/German-English/europarl-v7.de-en.de\", 'r', encoding='utf-8')]\n",
    "tgt_train_data = [line.strip() for line in open(\"data/alignment-scripts/train/German-English/europarl-v7.de-en.en\", 'r', encoding='utf-8')]\n",
    "masked_tgt_train_data = [generate_masked_sentences(sentence) for sentence in tgt_train_data]\n",
    "\n",
    "# Since I need different masks for each epoch, consider generating masks on-the-fly during training or generate multiple sets in advance.\n",
    "\n",
    "# Prepare data\n",
    "src_test_data = [\"<bos> \" + line.strip() for line in open(\"path_to_preprocessed_german_test_file.txt\", 'r', encoding='utf-8')]\n",
    "tgt_test_data = [line.strip() for line in open(\"path_to_preprocessed_english_test_file.txt\", 'r', encoding='utf-8')]\n"
   ],
   "id": "7b86c421b924b5c3",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combinations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 21\u001B[0m\n\u001B[1;32m     19\u001B[0m src_train_data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<bos> \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m line\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/alignment-scripts/train/German-English/europarl-v7.de-en.de\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n\u001B[1;32m     20\u001B[0m tgt_train_data \u001B[38;5;241m=\u001B[39m [line\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/alignment-scripts/train/German-English/europarl-v7.de-en.en\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n\u001B[0;32m---> 21\u001B[0m masked_tgt_train_data \u001B[38;5;241m=\u001B[39m [generate_masked_sentences(sentence) \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m tgt_train_data]\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Since I need different masks for each epoch, consider generating masks on-the-fly during training or generate multiple sets in advance.\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Prepare data\u001B[39;00m\n\u001B[1;32m     26\u001B[0m src_test_data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<bos> \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m line\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath_to_preprocessed_german_test_file.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n",
      "Cell \u001B[0;32mIn[1], line 21\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     19\u001B[0m src_train_data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<bos> \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m line\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/alignment-scripts/train/German-English/europarl-v7.de-en.de\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n\u001B[1;32m     20\u001B[0m tgt_train_data \u001B[38;5;241m=\u001B[39m [line\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/alignment-scripts/train/German-English/europarl-v7.de-en.en\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n\u001B[0;32m---> 21\u001B[0m masked_tgt_train_data \u001B[38;5;241m=\u001B[39m [\u001B[43mgenerate_masked_sentences\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentence\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m tgt_train_data]\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Since I need different masks for each epoch, consider generating masks on-the-fly during training or generate multiple sets in advance.\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Prepare data\u001B[39;00m\n\u001B[1;32m     26\u001B[0m src_test_data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<bos> \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m line\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath_to_preprocessed_german_test_file.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n",
      "Cell \u001B[0;32mIn[1], line 6\u001B[0m, in \u001B[0;36mgenerate_masked_sentences\u001B[0;34m(sentence, mask_token, max_outputs)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Assuming the first word is '<bos>' and should not be masked\u001B[39;00m\n\u001B[1;32m      5\u001B[0m num_to_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mint\u001B[39m((num_words \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m0.1\u001B[39m))  \u001B[38;5;66;03m# Exclude the first token from the percentage calculation\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m index_permutations \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[43mcombinations\u001B[49m(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, num_words), num_to_mask))  \u001B[38;5;66;03m# Start range at 1 to exclude '<bos>'\u001B[39;00m\n\u001B[1;32m      7\u001B[0m random\u001B[38;5;241m.\u001B[39mshuffle(index_permutations)  \u001B[38;5;66;03m# Shuffle to ensure random selection\u001B[39;00m\n\u001B[1;32m      9\u001B[0m masked_sentences \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mNameError\u001B[0m: name 'combinations' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Reproduce BTBA model\n",
    "### Architecture\n",
    "* Start with an existing model with an architecture as close as possible to the original transformer model (Vaswani et al., 2017)\n",
    "    * Used implementation existing in torch - https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "    * Maybe in the future:\n",
    "        * BART is used in this case for its sequence to sequence nature, and unlike BERT, relies on Decoder as well as pretrained with denoising autoencoder.\n",
    "* Remove the feed forward sub layer and the normalizations steps associated with it in the final decoder layer to access its final attention value and its output"
   ],
   "id": "87eaa9a6f9c23978"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:28:33.970453Z",
     "start_time": "2024-04-21T18:28:32.849527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Architecture dependencies - requires `torch`, `TensorFlow` >= 2.0 and `transformers`\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset"
   ],
   "id": "dae36055148238e7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class CustomTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_encoder_layers, num_decoder_layers, dropout=0.1):\n",
    "        super(CustomTransformerModel, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.src_tok_emb = nn.Embedding(input_dim, model_dim)\n",
    "        self.tgt_tok_emb = nn.Embedding(input_dim, model_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 512, model_dim))\n",
    "        self.output_linear = nn.Linear(model_dim, input_dim)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_key_padding_mask, tgt_key_padding_mask):\n",
    "        src_emb = self.src_tok_emb(src) + self.positional_encoding[:, :src.size(1), :]\n",
    "        tgt_emb = self.tgt_tok_emb(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n",
    "        memory = self.transformer_encoder(src_emb, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        return self.output_linear(outs)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.full((sz, sz), float('-inf'))\n",
    "        mask_cond = torch.arange(mask.size(-1))\n",
    "        mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "        return mask\n"
   ],
   "id": "60b13967459c1ff1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# Assume 'vocab_size' is determined after BPE and 'input_dim' equals 'vocab_size'\n",
    "model = CustomTransformerModel(input_dim=vocab_size, model_dim=512, num_heads=8, num_encoder_layers=6, num_decoder_layers=6)\n",
    "optimizer = Adam(model.parameters(), lr=0.0002)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Assuming dataloaders are set up with masked_tgt_train_data and src_train_data\n",
    "for epoch in range(50):  # Or as specified for each language pair\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in dataloader:\n",
    "        src_mask = model.generate_square_subsequent_mask(src.size(0))\n",
    "        tgt_mask = model.generate_square_subsequent_mask(tgt.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt, src_mask, tgt_mask, None, None)\n",
    "        loss = criterion(output.view(-1, vocab_size), tgt.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}: Loss {total_loss / len(dataloader)}\")\n"
   ],
   "id": "74788de8cbcc89f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import BartForConditionalGeneration, BartConfig\n",
    "\n",
    "class CustomBartModel(BartForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Assuming modifications are needed in attention or other specifics\n",
    "        for layer in self.model.decoder.layers:\n",
    "            layer.self_attn.self_attention.is_bidirectional = True\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask):\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            output_attentions=True  # Ensure attentions are returned for analysis\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "# Load pre-configured BART\n",
    "config = BartConfig.from_pretrained('facebook/bart-large')\n",
    "config.output_attentions = True  # Ensure we can access attentions for alignment analysis\n",
    "model = CustomBartModel(config)\n"
   ],
   "id": "b9d940f499e50810"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n",
    "# Tokenize data\n",
    "train_encodings = tokenizer(src_train_data, tgt_train_data, padding=True, truncation=True, max_length=512)\n",
    "test_encodings = tokenizer(src_test_data, tgt_test_data, padding=True, truncation=True, max_length=512)\n"
   ],
   "id": "28d4c0f65ffe342"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.0002)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(50):  # Adjust epochs per language pair requirements\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:  # Assuming dataloader is properly set up\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'],\n",
    "                        decoder_input_ids=batch['decoder_input_ids'], decoder_attention_mask=batch['decoder_attention_mask'])\n",
    "        loss = criterion(outputs.logits.view(-1, tokenizer.vocab_size), batch['labels'].view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}: Loss {total_loss / len(dataloader)}\")\n"
   ],
   "id": "d5bcec9b3dad6df1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def symmetrize_alignments(forward_alignments, backward_alignments):\n",
    "    \"\"\"\n",
    "    Symmetrize the alignments using a heuristic like grow-diagonal-final-and.\n",
    "    \n",
    "    forward_alignments: Dict[Tuple[int, int], float] - Alignment scores from source to target.\n",
    "    backward_alignments: Dict[Tuple[int, int], float] - Alignment scores from target to source.\n",
    "    \"\"\"\n",
    "    symmetrized_alignments = set()\n",
    "    max_threshold = 0.1  # Define a threshold to filter alignments by score\n",
    "\n",
    "    # Simple union symmetrization for demonstration\n",
    "    for (i, j), score in forward_alignments.items():\n",
    "        if score > max_threshold:\n",
    "            symmetrized_alignments.add((i, j))\n",
    "\n",
    "    for (j, i), score in backward_alignments.items():\n",
    "        if score > max_threshold:\n",
    "            symmetrized_alignments.add((i, j))\n",
    "\n",
    "    return symmetrized_alignments\n",
    "\n",
    "# Legend\n",
    "forward_alignments = { (1, 2): 0.9, (2, 3): 0.85 }\n",
    "backward_alignments = { (2, 1): 0.88, (3, 2): 0.90 }\n",
    "symmetrized = symmetrize_alignments(forward_alignments, backward_alignments)\n"
   ],
   "id": "e5d90d82c03e6b0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Assume spm_model is the path to the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor(model_file='spm_model.model')\n",
    "\n",
    "def encode_with_bpe(texts, sp):\n",
    "    return [sp.encode(text, out_type=str) for text in texts]\n",
    "\n",
    "# Example usage\n",
    "encoded_texts = encode_with_bpe([\"This is a sample text.\", \"Here is another one.\"], sp)\n",
    "\n",
    "\n",
    "src_train_data_bpe = encode_with_bpe(src_train_data, sp)\n",
    "tgt_train_data_bpe = encode_with_bpe(tgt_train_data, sp)\n",
    "\n",
    "# Assume data loaders and training setup are adjusted to use *_train_data_bpe\n"
   ],
   "id": "4e703307c53dd48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Concatenate all text files into one for SentencePiece training\n",
    "cat path_to_source_train_file.txt path_to_target_train_file.txt > full_text.txt\n",
    "\n",
    "# Train SentencePiece Model\n",
    "spm_train --input=full_text.txt --model_prefix=bpe_model --vocab_size=40000 --character_coverage=0.995 --model_type=bpe\n"
   ],
   "id": "7e785fa6fd6f5ca9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Load trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bpe_model.model')\n",
    "\n",
    "def encode_with_bpe(texts, sp):\n",
    "    return [sp.encode_as_pieces(text) for text in texts]\n",
    "\n",
    "# Load and encode training data\n",
    "src_train_data = [\"<bos> \" + line.strip() for line in open(\"path_to_preprocessed_german_train_file.txt\", 'r', encoding='utf-8')]\n",
    "tgt_train_data = [line.strip() for line in open(\"path_to_preprocessed_english_train_file.txt\", 'r', encoding='utf-8')]\n",
    "\n",
    "src_train_encoded = encode_with_bpe(src_train_data, sp)\n",
    "tgt_train_encoded = encode_with_bpe(tgt_train_data, sp)\n"
   ],
   "id": "286a793545f80dfe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Assuming a custom Transformer model setup as discussed\n",
    "from torch.optim import Adam\n",
    "optimizer = Adam(model.parameters(), lr=0.0002)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in zip(src_train_encoded, tgt_train_encoded):\n",
    "        src_tensor = torch.tensor([sp.piece_to_id(token) for token in src]).unsqueeze(1)  # Batch size 1 for simplicity\n",
    "        tgt_tensor = torch.tensor([sp.piece_to_id(token) for token in tgt]).unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src_tensor, tgt_tensor, None, None)  # Adjust masks and padding as necessary\n",
    "        loss = criterion(output.view(-1, len(sp)), tgt_tensor.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}: Loss {total_loss / len(src_train_encoded)}\")\n"
   ],
   "id": "b7d60e84ef473f85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example symmetrization function should be applied post-training with actual attention data\n",
    "symmetrized_alignments = symmetrize_alignments(forward_attention_data, backward_attention_data)\n"
   ],
   "id": "ff9f826318eed331"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
